# CUDA编程基础与实践

## 1. GPU硬件与CUDA程序开发工具

### 1.1 GPU硬件简介

GPU是英文缩写graphics processing unit，意为图像处理器，对应概念CPU为central proccessing unit的缩写，意为中央处理器

CPU和GPU的区别：

* CPU只拥有少数的快速计算核心，而GPU拥有几百到几千个不那么快速的计算核心
* CPU中更多的晶体管用于数据缓存和流程控制，GPU中更多晶体管用于算术逻辑单元

*GPU靠众多的计算核心来获得相对较高的计算性能*

GPU计算不是只单独的GPU计算，而是指CPU+GPU的异构计算，GPU必须在CPU的调度下才能完成特定任务。CPU和GPU分别称之为主机（host）和设备（device），通过PCIe总线进行连接

* FLOPS: 浮点数运算峰值，即每秒最多能执行的浮点数运算次数，表征计算性能的一个重要参数

* GPU内存带宽: 影响计算性能的另一个参数

* 显存: 显存容量也会制约应用程序的性能

### 1.2 CUDA程序开发工具

可以用于GPU编程的几种软件开发工具

* CUAD是Nvidia推出的GPU编程语言
  * CUDA提供两层API，分别为CUDA Driver API（底层）和CUDA Runtime API
  * 应用程序使用GPU：1.调用CUDA库；2.CUDA Runtime API；3.CUDA Driver API，其实最终都是通过CUDA Driver API调用GPU
  * 不同的GPU架构由不同的计算能力，一般由X.Y表征硬件架构的计算能力

* OpenCL是一个更为通用的为各种异构平台编写并行程序的框架，也是AMD的GPU的主要程序开发工具

* OpenACC是由多个公司共同开发的异构并行编程标准

### 1.3 使用命令检查与设置设备

CUDA常用设置命令

```bash
#查看显卡信息
nvidia-smi

#设置调用GPU编号
export CUDA_VISIBLE_DEVICES=id

#设置GPU的计算模式
sudo nvidia-smi -i GPU_ID -c 0	#默认模式
sudo nvidia-smi -i GPU_ID -c 1  #独占进程模式
```



## 2. CUDA中的线程组织

### 2.1 CUDA程序

CUDA程序编译器为nvcc工具，nvcc是C++编译器的超集

CUDA程序由主机代码和设备代码两部分组成，因为GPU的调用需要CPU的参与。主机通过核函数方式对设备进行调用

```C++
// 一个简单，典型的CUDA程序结构
int main(){
  
  主机代码
  核函数调用
  主机代码
  return 0;
}
```

### 2.2 核函数简介

CUDA中的核函数与C++函数类似，显著差别是核函数必须被限定词`__global__`修饰，并且核函数的返回值类型必须为空`void`，`void`和`__global__`的顺序可以交换

```C++
//第一种方式，常用
__global__ void function_name(){}

//第二种方式
void __global__ function_name(){}
```

主机调用核函数时需要制定设备中调用线程个数

核函数中线程的组织为若干线程块（thread block），全部线程块构成一个网格（grid），每个线程块含有相同数目的线程，数目为线程块的大小

```C++
//核函数调用方式
function_name<<<grid_size,block_size>>>();
```

调用核函数之后需要执行

```C++
cudaDeviceSynchronize();			//同步主机和设备
```

调用输出函数（printf）时，输出流时先存放在缓冲区，缓冲区不会自动刷新，只有程序遇到同步操作时缓存区才会刷新

### 2.3 CUAD中的线程组织

一个GPU一般有几千个计算核心，总线程数必须至少等于计算核心数才有可能充分利用GPU中的全部计算资源。实际上，总线程数大于计算核心时才能更充分地利用GPU中的计算资源，因为这会让计算和内存访问之间以及不同的计算之间合理的重叠，减小计算核心空闲时间

#### 2.3.1线程索引

```C++
function_name<<<grid_size,block_size>>>();
//grid_size表示网格大小最多2^31-1，block_size最大为1024
```

`grid_size`和`block_size`两个值分别保存在`gridDim`和`blockDim`两个内建变量中

另外，内建变量`blockIdx` : 线程在网格中的线程块索引；内建变量 `threadIdx`: 线程在线程块中的线程索引

四个内建变量都是多维的，可以通过`grid_size.x`方式访问各个维度，一维时，只有x维度可以访问

计算当前线程在所在线程块的ID：

thread_id=threadIdx.z\*blockDim.x\*blockDim.y+threadIdx.y\*blockDim.x+threadIdx.x

计算当前线程块在grid中的ID:

block_id=blockIdx.z\*gridDim.x\*gridDim.y+blockIdx.y\*gridDim.x+blockIdx.x

一个线程块中的线程还可以细分为不同的线程束。一个线程线程束是同一个线程块中相邻的warpSize个线程。warpSize也是内建变量，表示线程束的大小

#### 2.3.2 网格和线程块大小限制

grid_size三个维度不能大于2^31和65535和65535

block_size三个维度不能大于1024,1024,64，并且线程块的总大小不能超过1024线程

### 2.4 nvcc编译CUDA程序

CUDA的编译器驱动nvcc先将全部源代码分离为主机代码和设备代码。主机代码完整的支持C++语法，设备代码只部分支持C++语法。nvcc先将设备代码编译为PTX伪汇编代码，再将PTX代码编译为二进制的cubin目标代码。

* 将源代码编译为PTX代码时，需要选用-arch=compute_XY指定一个虚拟架构计算能力，用以确定代码中能够使用的CUAD功能。
* 将PTX代码编译为cubin代码时，需要用-code=sm_ZW指定一个真实架构的计算能力，用以确定可执行文件能够使用的GPU。真实架构的计算能力必须等于或大于虚拟架构的计算能力

```bash
-arch=compute_35 -code=sm_60
```

使用-code=sm_ZW指定GPU真实架构为Z.W。对应的可执行文件只能在主版本号为Z，次版本号大于或等于W的GPU中运行

如果希望编译出的可执行文件能够在更多的GPU中执行，可以同时指定多组计算能力

```bash
-gencode arch=compute_35,code=sm_35
-gencode arch=compute_60,code=sm_60
...
```

这样生成可执行文件称为胖二进制文件，因为包含了指定数量的二进制版本，在不同的架构中GPU运行时自动选择对应二进制版本。过多地指定计算能力，会增加编译时间和可执行文件的大小

另外，nvcc有一种机制称为即使编译，在可执行文件中保留一个如下的PTX代码

```bash
-gencode arch=compute_XY,code=compute_XY
```

简化的命令

```bash
-arch=sm_XY
```

上面代码等价于

```bash
-gencode arch=compute_XY,code=sm_ZW
-gencode arch=compute_XY,code=compute_XY
```

也可以不指定计算能力，此时编译时将采用默认计算能力（和CUDA版本和显卡型号相关）



## 3. CUDA程序的基本框架

### 3.1 CUDA程序的基本框架

```C++
头文件包含
常量定义（或宏定义）
C++自定义函数和CUDA核函数的声明
int main(){
  
  分配主机内存与设备内存
  初始化主机中的数据
  将某些数据从主机复制到设备
  调用核函数在设备中进行计算
  将某些数据从设备复制到主机
  释放主机与设备内存
}

C++自定义函数和核函数的定义
```

### 3.2设备内存的分配与释放

C++中malloc()函数动态分配内存，在CUDA中，设备内存的动态分配使用cudaMalloc()函数

```C++
//cudaMalloc函数原型
cudaError_t cudaMalloc(void **address,size_t size);
//返回值类型为cudaError_t错误代码，调用成功返回cudaSuccess，否则返回错误代码
//输入第一个参数:待分配设备内存的指针。注意：内存（地址）本身就是一个指针，所以待分配设备内存的指针就是双重指针
//第二个参数:带分配内存的字节数
```

cudaMalloc()函数分配的内存，必须使用cudaFree()函数进行释放

```c++
cudaError cudaFree(void* address);
```

主机与设备之间的数据传递

```c++
//将一定字节数的数据从源地址所指缓冲区复制到目标地址所指缓冲区
cudaError_t cudaMemcpy(void *dst,
                      const void *src,
                      size_t count,
                      enum cudaMemcpyKind kind
                      );
//dst是目标地址
//src是原地址
//count表示复制数据的字节数
//kind是枚举类型的变量，标志数据的传递方向，只能取如下几个值：
//1.cudaMemcpyHostToHost
//2.cudaMemcpyHostToDevice
//3.cudaMemcpyDeviceToHost
//4.cudaMemcpyDeviceToDevice
//5.cudaMemcpyDefault,表示根据指针dst和src所指地址自动推断数据传输方向（要求系统具有统一虚拟寻址功能，64位主机）
```

### 3.3 核函数要求

* 核函数返回类型必须是void，可以使用return，但是不能有返回值
* 必须使用限定符\_\_global\_\_，不影响添加C++其他限定符，例如static，限定符次序任意
* 函数名无要求，支持C++中的重载
* 不支持可变参数量的参数列表
* 可以向核函数传递非指针变量，其内容对每个线程可见
* 除非使用统一内存编译机制，否则传给核函数的数组（指针）必须指向设备内存
* 核函数不可称为一个类成员。通常做法是用一个包装函数调用核函数，而将包装函数定义为类成员
* 计算能力3.5之前，核函数之间不可以相互调用，之后引入动态并行机制，在核函数内部可以调用其他核函数，设置可以调用自己（递归）
* 无论从主机调用还是设备调用，核函数都在设备中运行，调用核函数必须指定执行配置，三括号及其中的参数

### 3.3 自定义设备函数

核函数可以调用不带执行配置的自定义函数，这样的自定义函数称为设备函数。它在设备中执行，并在设备中调用。与之相比，核函数在设备中执行，但是在主机端被调用

#### 3.3.1 函数执行空间标识符

CUDA程序中，以下标识符确定函数在哪里被调用，已在在哪里被执行

* \_\_global\_\_修饰的函数称为**核函数**，一般由**主机调用，在设备中执行**。如果使用动态并行，也可以在核函数中调用自己或其他核函数
* \_\_device\_\_修饰的函数称为**设备函数**，只能**被核函数**或**其他设备函数**调用，在设备中执行
* \_\_host\_\_修饰的函数就是**主机端的普通C++函数**，在主机中被调用，在主机中执行。对于主机端的函数，修饰符可以省略。可以**使用\_\_device\_\_和\_\_host\_\_同时修饰一个函数**，使得该函数既是C++中普通函数也是设备函数。可以减少代码的冗余。编译将针对主机和设备分别编译函数
* 不能使用_\_global\_\_和\_\_device\_\_同时修饰一个函数
* 不能使用\_\_host\_\_和\_\_global\_\_同时修饰一个函数
* 编译器决定把设备函数当作内联函数和非内联函数，但可以使用\_\_noinline\_\_建议一个设备函数为非内联函数（编译器不一定接受）
* 可以使用\_\_forceinline\_\_建议一个设备函数为内联函数



## 4. CUDA程序错误检测

### 4.1 检测CUDA运行时错误的宏函数

cudaMalloc(),cudaFree(),cudaMemcpy(),返回值类型都为cudaError_t，只有返回cudaSuccess的时候，才代表成功调用了API 函数。可以使用宏函数的方式，对API进行检查

```c++
#pragma once
#include <stdio.h>

#define checkCudaRuntime(call)																						\
do{                                                                   \
    const cudaError_t error_code=call;                                \
    if(error_code != cudaSuccess){																		\
        printf("CUDA Error:\n");																			\
        printf("File %s\n",__FILE__);																	\
        printf("Line %s\n",__LINE__);																	\
        printf("Error code: %d\n",error_code);												\
        printf("Error text:%s\n",cudaGetErrorString(error_code));			\
        exit(1);																											\
    }																																	\
}while(0)
```

#pragma once是一个预处理命令，确保当前头文件只在一个编译单元中不被重复

checkRuntime表示宏函数名称，参数call表示CUDA运行时API函数，宏函数一行写不下时，需要使用\符号进行续行

#### 4.1.1 检查运行时API函数

```c++
checkCudaRuntime(cudaRuntimeAPI(参数))
```

该函数可以检测cuda运行时API是否运行成功，有一个例外：cudaEventQuery()函数，其很有可能返回cudaErrorNotReady，但是不代表程序发生错误

#### 4.1.2 检查核函数

上诉方法不能捕捉嗲用调用核函数的相关错误，因为核函数不返回任何值。

捕捉调用核函数可能发生的错误：

```c++
checkCudaRuntime(cudaGetLastError());				//捕捉下面语句之前的最后一个错误
checkCudaRuntime(cudaDeviceSynchronize());	//同步主机和设备
```

同步主机和设备是因为核函数的调用是异步的，即主机发出调用核函数的命令后会立即执行后面语句，不会等待核函数执行完成。

需要注意，上诉同步函数比较耗时，如果在程序的较内层循环调用该同步函数，很可能严重降低程序的性能。所以一般不在较内层循环调用上诉同步函数。只要在核函数调用之后还有对其他任何能返回错误值API函数进行同步调用，都能触发主机与设备的同步并捕捉到核函数调用中可能发生的错误。

```bash
$ export CUDA_LAUNCH_BLOCKING=1							# 临时改变指定环境变量的值
```

这样设置之后所有核函数的调用都将不再是异步的，而是同步的。由于会严重影响程序性能，所以一般只在调试的时候进行设置。

### 4.2 用CUDA-MEMCHECK检查内存错误

CUDA提供了CUDA-MEMCHECK工具集，具体包括memcheck、racecheck、initcheck、synccheck共4个工具。可有可执行文件cuda-memcheck调用：

```bash
$ cuda-memcheck --tool memcheck [options] app_name [options]
$ cuda-memcheck --tool racecheck [options] app_name [options]
$ cuda-memcheck --tool initcheck [options] app_name [options]
$ cuda-memcheck --tool synccheck [options] app_name [options]
```

*[]中表示非必要参数*



## 5. 获取GPU加速的关键

前面关注程序的正确性，没有强调程序的性能。开发CUDA程序时，需要验证某些改变是否提高了程序的性能，这就需要对程序进行比较精确的计时

### 5.1 用CUDA事件计时

有效内存带宽：GPU在单位时间内访问设备内存的字节

CPU和GPU之间的数据传输时间是十分占用资源的，如果只是进行简单的数组相加，不仅不会提升性能而且会造成性能的下降，因为执行速度是由显存带宽决定的，而不是由浮点运算峰值决定的

CUDA工具箱中的nvprof工具，可以详细的剖析程序的性能（程序中各种操作花费的时间）

```bash
$ nvprof 可执行文件

# 当上面命令出现Unable to profile applicaton. Unfied Memory profiling failed错误时，使用下面命令
$ nvprof --unified-memory-profiling off 可执行文件
```

### 5.2 影响GPU加速的关键因素

#### 5.2.1 数据传输的比例

GPU计算核心和设备内存之间数据传输的峰值理论带宽要远高于GPU和CPU之间数据传输的带宽

要获得可观的GPU加速，就必须尽量缩减数据传输所花的时间比例。即使有些计算在GPU中加速并不高，也尽量在GPU中实现，避免过多的数据经过PCIe传递。这是CUDA编程中较重要的原则之一

#### 5.2.2 算术强度

计算问题的算术强度：算术操作的工具量与必要内存操作的工作量之比

对设备内存的访问速度取决于GPU的显存带宽

#### 5.2.3 并行规模

并行规模由GPU中总线程数目衡量。

GPU有多个流处理器（streaming multiprocessor,SM）构成，每一个SM中由若干CUDA核心。每个SM相对独立。如果一个核函数定义的线程数目远小于GPU支持的最大线程数，就很难得到很高的加速比

只有GPU满负荷工作情况下，GPU中的计算资源才能充分地发挥作用，从而获得较高的加速比

数据规模很小的问题，用GPU很难得到可观的加速

#### 5.2.4 总结

CUDA程序能够获得高性能的必要条件：

* 数据传输比例小
* 核函数的算术强度较高
* 核函数中定义的线程数目较多

编写优化CUDA程序时，注意下面几点：

* 减少主机与设备之间的数据传输
* 提高和核函数的算术强度
* 增大核函数并行规模

### 5.3 CUDA数学函数库

CUDA数学库中的函数：

* 单精度浮点数内建函数和数学函数。使用该类函数时不需要包含任何额外的头文件
* 双精度浮点数内建函数和数学函数。使用该类函数时不需要包含任何额外的头文件
* 半精度浮点数内建函数和数学函数。使用该类函数时需要包含头文件<cuda_fp16.h>
* 整数类型的内建函数，使用该类函数时不需要包含任何额外的头文件
* 类型转换内建函数，使用该类函数时不需要包含任何额外的头文件
* “单指令-多数据”内建函数（SIMD intrinsics），使用该类函数时不需要包含任何额外的头文件

CUDA中数学函数是经过重载的，内建函数是指准确度较低，但是效率较高的函数



## 6. CUDA的内存组织

### 6.1 CUDA的内存组织简介

计算机中内存存在一种组织架构。在这种结构中，含有多种类型的内存，每种内存分别具有不同的容量和延迟。一般来说延迟低的内存容量小，延迟高的内存容量大。当前被处理的数据一般存放于低延迟、低容量的内存中；当前没有被处理当之后将要被处理的大量数据一般存放在高延迟，高容量的内存中。相对于不用分级的内存，用这种分级的内存可以降低延迟，提高计算效率

CPU和GPU都存在内存分级设计，相对于CPU编程来说，GPU编程模型向我们提供了更多的控制权。因此，对CUDA编程来说，熟悉其内存的分级组织是非常重要的。

### 6.2 CUDA中不同类型的内存

#### 6.2.1 全局内存

核函数中所有的线程都能够访问其中的数据。全局内存不在GPU芯片上，所有具有较高的延迟和较低的访问速度。其容量是所有设备内存中最大的，基本与显存相当。全局内存可读可写

全局内存的主要角色是为核函数提供数据，并在主机与设备及设备与设备之间传递数据。可以使用`cudaMalloc()`函数为全局内存变量分配设备内存，然后可以在核函数中直接访问分配的内存，改变其中的数据值。可以使用`cudaMemcpy()`函数将主机内存的数据复制到全局内存，或则反过来。

```c++
// 将M字节的数据从主机复制到设备
cudaMemcpy(d_x,h_y,M,cudaMemcpyHostToDevice);
  
// 将M字节的数据从设备复制到主机
cudaMemcpy(h_y,d_y,M,cudaMemcpyDeviceToHost);

// 将全局内存中一段数据复制到全局内存,d_x,d_y表示内存首地址
cudaMemcpy(d_x,d_y,M,cudaMemcpyDeviceToDevice);
```

全局内存对于整个网络的所有线程可见，所有的线程可以访问传入核函数的设备指针所指向的全局内存中的全部数据。全局内存的生命周期不是由核函数决定，而是由主机端决定。生命周期由主机端使用`cudaMalloc()`函数分配内存开始，到主机端使用`cudaFree()`释放内存结束

以上所有的全局内存都称为线性内存。在CUDA中还有一种内部构造对用户不透明的全局内存，称为CUDA array（不对用户公开的数据排列方式，专为纹理拾取服务）

前面介绍的全局内存变量都是动态分配内存的。在CUDA中允许使用静态全局内存变量，其所占内存数量在编译期间就确定了，这样的静态全局内存变量必须在所有主机与设备函数外部定义

静态全局内存变量由以下方式在任何函数外部定义：

```c++
__device__ T x;			// 单个变量
__device__ T y[N];	// 固定长度的数组
```

在核函数中，可以直接对静态全局变量进行访问，并不需要将他们以参数的形式传给核函数。不可在主机函数中直接访问静态全局内存变量，但是可以使用`cudaMemcpyToSymbol()`函数和`cudaMemcpyFromSymbol()`函数在静态全局内存与主机内存之间传输数据，合理使用静态内存可以加速程序的

#### 6.2.2 常量内存

常量内存是有常量缓存的全局内存，数量有限。可以范围和生命周期与全局内存一样，常量内存只可读不可写。由于有缓存，常量内存的访问速度比全局内存高，但得到高访问速度的前提是一个线程束中的线程要读取相同的常量内存数据

常量内存的使用方法是在核函数外面用`__constant__`定义变量，并用前面介绍的CUDA运行时API函数`cudaMemcpyToSybol()`数据从主机端复制到设备的常量内存后供核函数使用

#### 6.2.3 纹理内存和表面内存

纹理内存和表面内存类似于常量内存，也是一种具有缓存的全局内存，有相同的可见范围和生命周期，一般仅可读。纹理内存和表面内存容量更大，使用方式和常量内存不同

#### 6.2.4 寄存器

在核函数中定义的不加任何限定符的变量一般来说就存在于寄存器中。核函数中定义不加任何限定符的数据可能存放在寄存器中，也可能存放在局部内存中。各种内建变量`gridDim,blockDim,blockIdx,threadIdx,warpSize`都存放在特殊的寄存器中。在核函数中访问寄存器是很高效的。寄存器变量仅被一个线程可见。寄存器的生命周期与所属线程生命周期一致。

寄存器位于芯片上，是所有内存中访问速度最高的。

#### 6.2.5 局部内存

在用法上看，局部内存和寄存器几乎一样。核函数中定义不加任何限定符的变量有可能在寄存器中，也有可能在局部内存中。寄存器中放不下的变量，以及索引值不能在编译时就确定的数组，都有可能放在局部内存中。从硬件上看，局部内存只在全局内存的一部分。所以全局内存的延迟也很高。

#### 6.2.6 共享内存

共享内存和寄存器类似，存在芯片上，具有仅次于寄存器的读取速度，数量也有限。不同于寄存器，共享内存对整个线程块可见，生命周期也与整个线程块一致。每个线程块拥有一个共享内存变量的副本，共享内存的值在不同的线程块中可以不同。一个线程块中的所有线程都可以访问该线程块的共享内存变量副本，但是不可以访问其他线程块的内存共享副本。共享内存的作用是减少对全局内存的访问次数，或改善对全局内存的访问模式

#### 6.2.7 L1和L2缓存

从费米架构开始，有了SM层次的L1缓存（一级缓存）和设备（一个设备有多个SM）层次的L2缓存。主要用来缓存全局变量和局部内存的访问，减少延迟

### 6.3 SM及其占有率

#### 6.3.1 SM的组成

一个GPU由多个SM构成，一个SM包含如下资源：

* 一定数量的寄存器
* 一定数量的共享内存
* 常量内存的缓存
* 纹理和表面内存的缓存
* L1缓存
* 两个或四个线程束调度器，用于在不同线程的上下文之间迅速的切换，以及为准备就绪的线程束发出执行指令
* 执行核心
  * 整数型核心
  * 单精度浮点数运算核心（FP32）
  * 双精度浮点数元算核心（FP64）
  * 单精度浮点数超越函数的特殊函数单元
  * 混合精度的张量核心

### 6.4 CUDA运行时API查询设备参数









